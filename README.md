## Introduction to Word Embeddings

This repository is aimed to provide an abstract level introduction to different Word Embeddings in NLP.

<!-- * [word_embs.ipynb](./embeddings.ipynb) file contains the code with explanation for the tutorial. -->
* [WordEmbeddings.md](./WordEmbeddings.md) is the lecture note for the python notebook codes.

### How to run the code

To run the code, follow these steps:

1. `git clone https://github.com/panditu2015/Word-Embeddings.git`
2. `cd Word-Embeddings`
3. `pip install -r requirements.txt`
4. `jupyter notebook`
5. Open [word_embs.ipynb](./embeddings.ipynb) in your browser now.

### Resources

* Most of the code is written using `pymagnitude` library in Python.

* **Stanford's CS276**: [Information Retrieval and Web Search](http://web.stanford.edu/class/cs276/)

* **Stanford's CS224n**: [Natural Language Processing with Deep Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
    * Useful lectures: 1 (Jan 8), 2 (Jan 10), 13 (Feb 19)

* **Stanford's CS224U**: [Natural Language Understanding](http://web.stanford.edu/class/cs224u/)
    * Useful lectures: 2 (Apr 8), 7 (May 11)

* A really nice blog on the intuition of Word Embeddings: 
    - [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)
    - [Video version of this blog](http://jalammar.github.io/skipgram-recommender-talk/).

* A great blog on Contextual Embeddings:
    - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/).

* Lecture on [Distributed Representations](http://web.stanford.edu/class/cs276/19handouts/lecture14-distributed-representations-1per.pdf) from CS276.

* A lecture note on [Word Embeddings](https://arxiv.org/pdf/1902.06006.pdf) from CS224n.