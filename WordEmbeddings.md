## Introduction

Word Embeddings are used to represent words in a multi-dimensional vector form. A word <span class="math"><b>w<sub>i</sub></b></span> in vocabulary **V** is represented in the form of a vector of **n** dimensions. These vectors are generated by unsupervised training on a large corpus of words to gain the semantic similarities between the words. Algorithms like word2vec and GloVe are used to train these word embeddings. These word embeddings are generally pre-trained and made publicly available to be directly used in the deep learning models.

### Intuition

## Static Word Embeddings

### Word2Vec
### GloVe
### fastText

## Contextual Word Embeddings

### ELMo
### BERT
#### Tokenizer
#### Embedder